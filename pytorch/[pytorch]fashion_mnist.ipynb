{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "planned-saskatchewan",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial - Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sublime-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "proved-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "institutional-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "training_epochs = 16\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "individual-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.FashionMNIST(root='FashionMNIST/', # 다운로드 경로 지정\n",
    "                          train=True, # True를 지정하면 훈련 데이터로 다운로드\n",
    "                          transform=transforms.ToTensor(), # 텐서로 변환\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.FashionMNIST(root='FashionMNIST/', # 다운로드 경로 지정\n",
    "                         train=False, # False를 지정하면 테스트 데이터로 다운로드\n",
    "                         transform=transforms.ToTensor(), # 텐서로 변환\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "formal-folder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "latest-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 전체 데이터 dataloader 생성시\n",
    "# data_loader = DataLoader(dataset=mnist_train,\n",
    "#                         batch_size=batch_size,\n",
    "#                         shuffle=True,\n",
    "#                         drop_last=True) #drop last ~ batch 단위로 데이터를 불러올 때 마지막 batch의 크기가 다를 경우 나머지의 해당 배치 데이터를 drop하는 option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clear-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "train_dataset, val_dataset = random_split(mnist_train, [50000,10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vanilla-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "timely-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_basic(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_basic, self).__init__()\n",
    "        self.keep_prob = 0.5 # drop out ratio\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 64, kernel_size=3, stride=1 , padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1 , padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1 , padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(4*4*128, 256, bias = True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        \n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=1-self.keep_prob)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(256, 10, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1) #Flatten for fully connected layer\n",
    "        out = self.layer4(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "radio-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = CNN_basic().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dimensional-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "settled-cradle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 배치의 수 : 781\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(train_loader)\n",
    "print('총 배치의 수 : {}'.format(total_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "short-updating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sitting-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / batch 0 / loss 2.293971061706543\n",
      "epoch 1 / batch 100 / loss 0.9591710567474365\n",
      "epoch 1 / batch 200 / loss 0.6048954725265503\n",
      "epoch 1 / batch 300 / loss 0.38689643144607544\n",
      "epoch 1 / batch 400 / loss 0.4305373728275299\n",
      "epoch 1 / batch 500 / loss 0.40433090925216675\n",
      "epoch 1 / batch 600 / loss 0.2554273307323456\n",
      "epoch 1 / batch 700 / loss 0.5498675107955933\n",
      "validation loss 0.4058389961719513\n",
      "=====================================\n",
      "epoch 2 / batch 0 / loss 0.5841600298881531\n",
      "epoch 2 / batch 100 / loss 0.23906071484088898\n",
      "epoch 2 / batch 200 / loss 0.23192040622234344\n",
      "epoch 2 / batch 300 / loss 0.4296759366989136\n",
      "epoch 2 / batch 400 / loss 0.40734267234802246\n",
      "epoch 2 / batch 500 / loss 0.35348930954933167\n",
      "epoch 2 / batch 600 / loss 0.35246437788009644\n",
      "epoch 2 / batch 700 / loss 0.2690242826938629\n",
      "validation loss 0.3379691541194916\n",
      "=====================================\n",
      "epoch 3 / batch 0 / loss 0.435079425573349\n",
      "epoch 3 / batch 100 / loss 0.2619270980358124\n",
      "epoch 3 / batch 200 / loss 0.23060572147369385\n",
      "epoch 3 / batch 300 / loss 0.22528234124183655\n",
      "epoch 3 / batch 400 / loss 0.30586186051368713\n",
      "epoch 3 / batch 500 / loss 0.18183888494968414\n",
      "epoch 3 / batch 600 / loss 0.35710564255714417\n",
      "epoch 3 / batch 700 / loss 0.3020888566970825\n",
      "validation loss 0.2975412905216217\n",
      "=====================================\n",
      "epoch 4 / batch 0 / loss 0.19609320163726807\n",
      "epoch 4 / batch 100 / loss 0.2831435799598694\n",
      "epoch 4 / batch 200 / loss 0.3463827967643738\n",
      "epoch 4 / batch 300 / loss 0.37432152032852173\n",
      "epoch 4 / batch 400 / loss 0.18096467852592468\n",
      "epoch 4 / batch 500 / loss 0.24381068348884583\n",
      "epoch 4 / batch 600 / loss 0.1894981861114502\n",
      "epoch 4 / batch 700 / loss 0.21353524923324585\n",
      "validation loss 0.2689956724643707\n",
      "=====================================\n",
      "epoch 5 / batch 0 / loss 0.3116370141506195\n",
      "epoch 5 / batch 100 / loss 0.17035123705863953\n",
      "epoch 5 / batch 200 / loss 0.24470505118370056\n",
      "epoch 5 / batch 300 / loss 0.24471265077590942\n",
      "epoch 5 / batch 400 / loss 0.18215928971767426\n",
      "epoch 5 / batch 500 / loss 0.23444829881191254\n",
      "epoch 5 / batch 600 / loss 0.30413907766342163\n",
      "epoch 5 / batch 700 / loss 0.2073223739862442\n",
      "validation loss 0.26725366711616516\n",
      "=====================================\n",
      "epoch 6 / batch 0 / loss 0.23070937395095825\n",
      "epoch 6 / batch 100 / loss 0.18159739673137665\n",
      "epoch 6 / batch 200 / loss 0.11212700605392456\n",
      "epoch 6 / batch 300 / loss 0.299089640378952\n",
      "epoch 6 / batch 400 / loss 0.11181055009365082\n",
      "epoch 6 / batch 500 / loss 0.1976105272769928\n",
      "epoch 6 / batch 600 / loss 0.1445191204547882\n",
      "epoch 6 / batch 700 / loss 0.2393132746219635\n",
      "validation loss 0.27058881521224976\n",
      "=====================================\n",
      "epoch 7 / batch 0 / loss 0.19773223996162415\n",
      "epoch 7 / batch 100 / loss 0.12582740187644958\n",
      "epoch 7 / batch 200 / loss 0.19441184401512146\n",
      "epoch 7 / batch 300 / loss 0.23017260432243347\n",
      "epoch 7 / batch 400 / loss 0.27550601959228516\n",
      "epoch 7 / batch 500 / loss 0.2584477365016937\n",
      "epoch 7 / batch 600 / loss 0.14956139028072357\n",
      "epoch 7 / batch 700 / loss 0.1658197045326233\n",
      "validation loss 0.24810749292373657\n",
      "=====================================\n",
      "epoch 8 / batch 0 / loss 0.13848471641540527\n",
      "epoch 8 / batch 100 / loss 0.17150099575519562\n",
      "epoch 8 / batch 200 / loss 0.24672174453735352\n",
      "epoch 8 / batch 300 / loss 0.08163981884717941\n",
      "epoch 8 / batch 400 / loss 0.07465006411075592\n",
      "epoch 8 / batch 500 / loss 0.2573124170303345\n",
      "epoch 8 / batch 600 / loss 0.1816178858280182\n",
      "epoch 8 / batch 700 / loss 0.2073688507080078\n",
      "validation loss 0.23271454870700836\n",
      "=====================================\n",
      "epoch 9 / batch 0 / loss 0.18108037114143372\n",
      "epoch 9 / batch 100 / loss 0.1688804030418396\n",
      "epoch 9 / batch 200 / loss 0.10282015055418015\n",
      "epoch 9 / batch 300 / loss 0.1406468003988266\n",
      "epoch 9 / batch 400 / loss 0.10437265038490295\n",
      "epoch 9 / batch 500 / loss 0.21691808104515076\n",
      "epoch 9 / batch 600 / loss 0.1516418308019638\n",
      "epoch 9 / batch 700 / loss 0.1254786103963852\n",
      "validation loss 0.24476546049118042\n",
      "=====================================\n",
      "epoch 10 / batch 0 / loss 0.18530093133449554\n",
      "epoch 10 / batch 100 / loss 0.11963760107755661\n",
      "epoch 10 / batch 200 / loss 0.10423669219017029\n",
      "epoch 10 / batch 300 / loss 0.19027884304523468\n",
      "epoch 10 / batch 400 / loss 0.2582562267780304\n",
      "epoch 10 / batch 500 / loss 0.07449331879615784\n",
      "epoch 10 / batch 600 / loss 0.0912255272269249\n",
      "epoch 10 / batch 700 / loss 0.13908296823501587\n",
      "validation loss 0.2401570975780487\n",
      "=====================================\n",
      "epoch 11 / batch 0 / loss 0.12801380455493927\n",
      "epoch 11 / batch 100 / loss 0.14088788628578186\n",
      "epoch 11 / batch 200 / loss 0.0372452437877655\n",
      "epoch 11 / batch 300 / loss 0.09768341481685638\n",
      "epoch 11 / batch 400 / loss 0.07103698700666428\n",
      "epoch 11 / batch 500 / loss 0.16003189980983734\n",
      "epoch 11 / batch 600 / loss 0.09244808554649353\n",
      "epoch 11 / batch 700 / loss 0.1721651703119278\n",
      "validation loss 0.26823145151138306\n",
      "=====================================\n",
      "epoch 12 / batch 0 / loss 0.11757349222898483\n",
      "epoch 12 / batch 100 / loss 0.09845112264156342\n",
      "epoch 12 / batch 200 / loss 0.12651856243610382\n",
      "epoch 12 / batch 300 / loss 0.04812747240066528\n",
      "epoch 12 / batch 400 / loss 0.08875102549791336\n",
      "epoch 12 / batch 500 / loss 0.11340838670730591\n",
      "epoch 12 / batch 600 / loss 0.20950019359588623\n",
      "epoch 12 / batch 700 / loss 0.08003079891204834\n",
      "validation loss 0.24992650747299194\n",
      "=====================================\n",
      "epoch 13 / batch 0 / loss 0.16143496334552765\n",
      "epoch 13 / batch 100 / loss 0.2324744313955307\n",
      "epoch 13 / batch 200 / loss 0.07698847353458405\n",
      "epoch 13 / batch 300 / loss 0.1705140769481659\n",
      "epoch 13 / batch 400 / loss 0.1328948587179184\n",
      "epoch 13 / batch 500 / loss 0.057961951941251755\n",
      "epoch 13 / batch 600 / loss 0.20728205144405365\n",
      "epoch 13 / batch 700 / loss 0.10153945535421371\n",
      "validation loss 0.23830045759677887\n",
      "=====================================\n",
      "epoch 14 / batch 0 / loss 0.11775685101747513\n",
      "epoch 14 / batch 100 / loss 0.02772141993045807\n",
      "epoch 14 / batch 200 / loss 0.13399940729141235\n",
      "epoch 14 / batch 300 / loss 0.04033153876662254\n",
      "epoch 14 / batch 400 / loss 0.06453286111354828\n",
      "epoch 14 / batch 500 / loss 0.18013867735862732\n",
      "epoch 14 / batch 600 / loss 0.1789456456899643\n",
      "epoch 14 / batch 700 / loss 0.046422623097896576\n",
      "validation loss 0.26187464594841003\n",
      "=====================================\n",
      "epoch 15 / batch 0 / loss 0.04879032075405121\n",
      "epoch 15 / batch 100 / loss 0.1539009064435959\n",
      "epoch 15 / batch 200 / loss 0.10382585972547531\n",
      "epoch 15 / batch 300 / loss 0.09811223298311234\n",
      "epoch 15 / batch 400 / loss 0.08673647791147232\n",
      "epoch 15 / batch 500 / loss 0.15618929266929626\n",
      "epoch 15 / batch 600 / loss 0.08468782901763916\n",
      "epoch 15 / batch 700 / loss 0.25455233454704285\n",
      "validation loss 0.26311662793159485\n",
      "=====================================\n",
      "epoch 16 / batch 0 / loss 0.10351544618606567\n",
      "epoch 16 / batch 100 / loss 0.07028725743293762\n",
      "epoch 16 / batch 200 / loss 0.1320987194776535\n",
      "epoch 16 / batch 300 / loss 0.11251872032880783\n",
      "epoch 16 / batch 400 / loss 0.13558927178382874\n",
      "epoch 16 / batch 500 / loss 0.05497102811932564\n",
      "epoch 16 / batch 600 / loss 0.04754747450351715\n",
      "epoch 16 / batch 700 / loss 0.19768892228603363\n",
      "validation loss 0.26591727137565613\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = [] \n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for j, (X, y) in enumerate(train_loader):\n",
    "        X= X.to(device)\n",
    "        y= y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model_ft(X)\n",
    "        cost = criterion(hypothesis, y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "        \n",
    "        if j%100 == 0:\n",
    "            print(\"epoch {} / batch {} / loss {}\".format(epoch+1, j, cost.data))\n",
    "    train_loss_history.append(avg_cost)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val= X_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            y_val_pred = model_ft(X_val)\n",
    "            v_loss = criterion(y_val_pred, y_val)\n",
    "            val_loss += v_loss / len(val_loader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        print(\"validation loss {}\".format(val_loss))\n",
    "        print('=====================================')\n",
    "        \n",
    "    model_ft.train()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "flexible-notification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 10000 images: 92.14%\n"
     ]
    }
   ],
   "source": [
    "model_ft.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_ft(imgs)\n",
    "        _, argmax = torch.max(outputs, 1)\n",
    "        total += imgs.size(0)\n",
    "        correct += (labels == argmax).sum().item()\n",
    "    \n",
    "    print('Accuracy for {} images: {:.2f}%'.format(total, correct / total * 100))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-disaster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
